{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "norman-sterling",
   "metadata": {},
   "source": [
    "# Investing.com News Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-winning",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-terry",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import requests\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "terminal-stack",
   "metadata": {},
   "source": [
    "## Define Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-serbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsNode:\n",
    "    def __init__(self, title, link, author, date, content):\n",
    "        self.title = title\n",
    "        self.link = link\n",
    "        self.author = author\n",
    "        self.date = date\n",
    "        self.content = content\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"{self.title}\\n- by {self.author} [{self.date.strftime('%Y-%m-%d')}]\\n\" + \\\n",
    "            f\"{self.link}\\n\" + f\"{self.content}\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriental-canada",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsCollection:\n",
    "    def __init__(self, company):\n",
    "        self.company = company\n",
    "        self.news = []\n",
    "        \n",
    "    def news_count(self):\n",
    "        return len(self.news)\n",
    "    \n",
    "    def add_news(self, news_node):\n",
    "        assert isinstance(news_node, NewsNode)\n",
    "        self.news.append(news_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsScraper:\n",
    "    \n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_date(date_str):\n",
    "        if len(date_str) == 0:\n",
    "            return None\n",
    "        \n",
    "        date_str = date_str[3:]\n",
    "        \n",
    "        if \"ago\" in date_str:\n",
    "            nhour = int(date_str.split()[0])\n",
    "            current_time = datetime.datetime.now()\n",
    "            publish_time = current_time - datetime.timedelta(hours=nhour)\n",
    "            \n",
    "            return datetime.datetime(publish_time.year, publish_time.month, publish_time.day)\n",
    "        \n",
    "        dt = datetime.datetime.strptime(date_str, \"%b %d, %Y\")\n",
    "        return datetime.datetime(dt.year, dt.month, dt.day)\n",
    "    \n",
    "    @classmethod\n",
    "    def fetch(cls, company, stop_date=None, verbose=False):\n",
    "        assert isinstance(stop_date, datetime.datetime)\n",
    "        \n",
    "        news_collection = NewsCollection(company)\n",
    "        stop_flag = False\n",
    "    \n",
    "        link = f\"https://www.investing.com/equities/{company}-news\"\n",
    "        response = requests.get(link, headers=cls.headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            # Success\n",
    "            soup = bs(response.content)\n",
    "\n",
    "            # Build NewsNode\n",
    "            page_links = soup.find_all(\"a\", {\"class\": \"pagination\"})\n",
    "\n",
    "            n_results = int(page_links[1]['title'].split()[-1])\n",
    "            n_pages = math.ceil(n_results / 10)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Processing news for {company} [since {stop_date.strftime('%Y-%m-%d')}]\")\n",
    "\n",
    "            for pc in range(1, n_pages + 1):\n",
    "                # Should the processing stop?\n",
    "                if stop_flag:\n",
    "                    break\n",
    "                \n",
    "                link = f\"https://www.investing.com/equities/{company}-news/{pc}\"\n",
    "\n",
    "                if verbose:\n",
    "                    if pc % 10 == 0 or pc == n_pages:\n",
    "                        print(f\"  Processing page {pc}...\")\n",
    "\n",
    "                if pc > 1:\n",
    "                    response = requests.get(link, headers=headers)\n",
    "\n",
    "                    if response.status_code != 200:\n",
    "                        if verbose:\n",
    "                            print(f\"Request Failed [{response.status_code}]: {link}\")\n",
    "                        continue\n",
    "                    else:\n",
    "                        soup = bs(response.content)\n",
    "\n",
    "                news_list = soup.select(\"#leftColumn .textDiv\")\n",
    "\n",
    "                for news in news_list:\n",
    "                    news_a = news.find_all('a')[0]\n",
    "                    news_title = news_a['title']\n",
    "                    news_link = news_a['href']\n",
    "                    \n",
    "                    if news_link[0] == '/':\n",
    "                        news_link = \"https://www.investing.com\" + news_link\n",
    "\n",
    "                    news_div = news.select(\".articleDetails\")[0]\n",
    "                    news_author = news_div.find_all('span')[0].contents\n",
    "                    if len(news_author) == 0:\n",
    "                        news_author = \"\"\n",
    "                    else:\n",
    "                        news_author = news_author[0]\n",
    "                    news_author = news_author.replace(\"By \", \"\")\n",
    "                    \n",
    "                    news_date = news_div.select(\".date\")[0].contents\n",
    "                    if len(news_date) == 0:\n",
    "                        news_date = \"\"\n",
    "                    else:\n",
    "                        news_date = news_date[0]\n",
    "                    news_date = NewsScraper.parse_date(news_date)\n",
    "                    \n",
    "                    if news_date is not None and news_date < stop_date:\n",
    "                        stop_flag = True\n",
    "                        break\n",
    "\n",
    "                    news_content = news.find_all('p')[0].contents\n",
    "                    if len(news_content) == 0:\n",
    "                        news_content = \"\"\n",
    "                    else:\n",
    "                        news_content = news_content[0]\n",
    "\n",
    "                    news_node = NewsNode(news_title, news_link, news_author, news_date, news_content)\n",
    "                    news_collection.add_news(news_node)\n",
    "                    \n",
    "            if verbose:\n",
    "                print(f\"Done. {news_collection.news_count()} items retrieved.\")\n",
    "                print()\n",
    "                    \n",
    "            return news_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-grave",
   "metadata": {},
   "source": [
    "## Fetch News for Companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-replacement",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = [\n",
    "    \"pepsico\",\n",
    "    \"disney\",\n",
    "    \"american-airlines-group\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-connectivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_lib = []\n",
    "\n",
    "dt = datetime.datetime(2011, 1, 1)\n",
    "for company in companies:\n",
    "    news_collection = NewsScraper.fetch(company, stop_date=dt, verbose=True)\n",
    "    news_lib.append(news_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-wages",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_lib[0].news[-1].date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-connectivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    print(news_lib[0].news[i])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
